---------1.7.3-----------
Dictionary 添加 finalize 方法。修正 tomcat reload 时 OOM 的 bug: http://code.google.com/p/mmseg4j/issues/detail?id=4
MMSegTokenizer 在 lucene 2.4 编译的 在 lucene 2.9 中会报 java.lang.NoSuchFieldError: input。bug: http://code.google.com/p/mmseg4j/issues/detail?id=5
---------1.7.2-----------
1.添加 lowerCaseFilter 后的一个 bug: NullPointerException
2.核心程序与 lucene 和 solr 扩展分开打包, 同时给出低版本的 lucene 扩展(lucene 1.9 到 2.2; lucene 2.3)
---------1.7.1-----------
没有此版本. 为了保存 1.6 系同版本小号
---------1.7-------------
1.删除没必要方法
2.analyzer 添加 LowerCaseFilter
---------1.7-beta--------
1.要比较的词不从 char[] sen 里复制,直接与词库结构比较, 性能提升10% 
2.用 key tree 的词库数据结构, 性能提升不少
3.用 key tree 里实现的 maxmatch, 同时返回所有相关词的长度, 性能提升不少
---------1.6-------------
1.实现多分词
2.允许多个词库文件
3.单字的单位独立一个文件(data/units.dic, 已经放入jar包里)
---------1.5-------------
1.使用sogou核心词库(15W)
2.把chars.dic文件放到jar里, 我们不需要关心他
3.最长匹配遍历调整(基本不受长词的影响)
4.优化了程序,除去没有必要的数组复制等,性能提升40%
---------1.0.2------------
1.数字接着的年月日独立分
---------1.0.1------------
1.MMSeg.next() 断句有个 bug。
空白字符后面的英文会丢失，且分词停止。

如：“手机电子书 http”空格后面的http丢了。

---------1.0--------------
1.实现 mmseg 算法分词
2.有两种 Simple 和 Complex 分词
3.扩展 Lucene 的 Analyzer, 以便结合 Lucene 使用
4.扩展 Solr 的 TokenizerFactory, 以便结合 Solr 使用